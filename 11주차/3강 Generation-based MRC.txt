1. Generation-based MRC
  주어진 지문과 질의를 보고 답변을 생성
  → 생성 문제

  평가 방법
  Extraction-based MRC와 동일한 평가 방법을 사용
  → EM, F1 Score

  Generation-based MRC & Extraction-based MRC 비교
  모델 구조
   - seq-to-seq PLM구조(generation)
   - PLM + Classifier 구조(extraction)

2. Pre-processing

Tokenization 
→ Special Token
   PAD 토큰은 그대로 사용
  / CLS, SEP 토큰은 자연어를 이용하여 정해진 텍스트 포맷으로 데이터를 생성
→ Attention mask
→ Token type ids
   입력 시퀀스에 대한 구분이 없어 사용하지 않음
   (BART 모델에서는 실제 tokenize 결과에 존재하지 않음)
→ 출력 표현
   전체 시퀀스의 각 위치마다 모델이 아는 모든 단어들 중
   하나의 단어를 맞추는 classification문제

3. Model
  BART
   기계 독해, 기계 번역, 요약, 대화 등 sequence to sequence
  문제의 pre-training을 위한 denoising autoencoder

  BART Encoder & Decoder
    BART의 인코더는 BERT처럼 bi-directional
    BART의 디코더는 GPT처럼 uni-directional(autoregressive)

  Pre-training BART
   BART는 텍스트에 노이즈를 주고 원래 텍스트를 복구하는
  문제를 푸는 것으로 pre-training 한다.

4. Post-processing
  Decoding
   디코더에서 이전 스템에서 나온 출력이 다음 스템의 입력으로 들어감(autoregressive)
  (맨 처음 입력은 문장 시작을 뜻하는 스페셜 토큰)

  Searching
   - Greedy Search
   - Exhaustive Search
   - Beam Search


