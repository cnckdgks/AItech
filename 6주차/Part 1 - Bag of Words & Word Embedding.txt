(01강) Intro to NLP, Bag-of-Words

강의 소개
Bag-of-Words with  one-hot-encoding
→ Naive Bayes Classifier



강의 영상

○ 자연어 처리의 분야
→ 자연어를 이해하는 NLU
→ 자연어를 상황에 따라 적절히 생성할 수 있는 NLG

○ 자연어 관련 딥러닝 기술, 구체적인 어플리케이션
→ language modeling, machine translation, 
  question answering, document classification



◎ 자연어 분야

○ NLP
자연어를 다루는 분야에는 여러가지가 있다.
→ NLP는 가장 발전이 활발한 분야이다.

- Low-level parsing : Tokenization, stemming
- Word and phrase level : NER, POS, noun-phrase chunking
  ,dependency parsing, conference resolution
- Sentence level : Sentiment analysis, machine translation
- Multi-sentence and paragragh level : Entailment prediction,
  question answering, dialog systems, summarization


○ Text mining
- text, document로부터 유용한 insight나 정보를 추출
- Document clustering(e.g., topic modeling)
- Highly related to computational social science


○ Information retrieval
- Highly related to computational social science
→ 추천시스템과 매우 연관성 있다.

◎ Trends of NLP
- 단어 → 벡터 (Word2vec, GloVe)
- RNN-family models
- attention and transformer
- customized models for different NLP tasks
- self-supervised training setting that does not require additional labels
- transfer learning





(02강) Word Embedding

강의 소개
단어를 벡터로 표현하는 방법 : Word2Vec과 GloVe
→ 단어의 distributed representation을 학습하고자 고안된 모델


Further Questions
Word2Vec과 GloVe의 장단점?


◎ Bag-of Words 

○ Bag-of Words Representation
→ 문장을 토큰화하고 사전 크기에 맞게 각 단어를 one-hot-encoding
→ 각 단어 벡터를 모두 더한다.
 

○ NaiveBayes Classifier for Documenatation
   (For a document d and a class c)
→ 특정 문서가 주어졌을 때 클래스 분류하기
    = 특정 클래스가 주어졌을 때 해당 문서가 나올 확률 * 특정 클래스가 나올 확률


◎ Word Embedding
단어 → Word Embedding Model → 벡터
* Word Embedding Model은 단어간의 내용 유사성을 반영하도록 한다.

○ Word2Vec
주요 아이디어 : 특정 단어의 주변의 단어와의 관계가 높아지도록 embedding된다.
과정 : 특정 단어를 input, 특정 단어의 주변 단어들의 확률분포가 y값으로 주어지고
이를 최적화하도록 학습한다.
→ 이때의 input weight vector, output weight vector가 특정 단어의 embedding vector의 결과로 사용된다.

○ Glove
주요 아이디어 : Word2Vec에서 특정 단어 뒤에 나오는 단어가 여러번 나올 경우
여러번의 중복된 연산을 하게되는데 이러한 중복 연산을 피하도록 하는 방법
* Word2Ve과 큰 성능 차이가 없다.







