(07강) Transformer(1)

강의 소개
Transformer(self-attention)
→ Attention만을 이용해 입력 문장/단어의 representation을 학습하며
  좀 더 parallel한 연산이 가능하는 동시에 학습 속도가 빠르다.


강의 영상

RNN의 Long Term Dependency 문제
입력 문장의 길이가 길어지면 뒤로 갈수록 히든벡터에 앞의 정보가 반영되지 않는 문제가 발생
→ Bidirectional RNN으로 이러한 문제를 해결하고자 했다. 하지만 근본적인 해결 방안으로 제시된 것은 Transformer이다.

Transformer 작동 방식
- 입력벡터(x1, x2, x3...)에 3가지 linear transformation적용
→ key vector, query vector, value vector가 나온다.
- 하나의 query vector와 key vector의 내적 후 softmax를 취하여 가중치 값을 얻는다.
- value vector에 가중치 값을 적용하여 하나의 히든 벡터를 얻는다.


Scaled Dot -Product Attention (Attention의 행렬 표현)
A(Q, K, V) = softmax(QK^T/root dk)V
→ 여기서 dk는 QK^T과정에서 분산값이 커지는 것을 방지한다.
→ (a, b, c)(x, y, z)^T에서 각 원소가 평균 0, 분산 1이라고 가정하면
내적의 결과로 평균 0, 분산 3이 된다. 분산이 크면 softmax의 값이 한쪽에 치우칠 수 있다.
이를 방지하기 위해 root 3으로 나누어준다.



(08강) Transformer(2)

강의 소개
Transformer(Self-Attention)에 대해 이어서 자세히 알아본다.

Further Question
ttention을 모델의 Output을 설명하는 방법?

강의 영상

Multi-head Attention
V, K, Q 입력을 여러 개의 Attention 모듈에 적용한 뒤 결과를 
concat 하여 Linear layer에 통과한다. (Attention에서 나온 결과를 head라고 한다.)
→ 같은 문장에서도 서로 다른 측면의 정보가 추출 되도록한 뒤
이것이 취합되도록 하는 효과를 얻는다.


Multi-head Attention의 메모리 요구량
Self-Attention → n^2*d → 1 
(병렬화 가능; 다만 RNN보다 연산 요구량은 더 크다.)
RNN → n*d^2 → n
(병렬화 불가능; 이전 스텝의 계산이 다 끝날 때까지 기다려야한다.)


Transformer : Block-Based Model
- residual connection
→ attention의 결과값 + 입력값 (gradient vanishing 문제를 해결해준다.)
- normalization
batch normalization : 배치를 layer 통과 후 결과 값에 대해 normalization 후 Affine 변환(y = ax + b)한다.
layer normalization : 입력값 하나에 대해 normalization 후 Affine 변환한다.
→ 학습 안정화와 성능을 향상시킨다.

- *N : 같은 인코더 Block을 직렬로 N번 적용한다.
→ 정보가 고도화되는 효과를 가져온다.


Transformer : Positional Encoding
입력값의 순서에 대한 정보를 부여하는 방식
→ 주로 삼각함수를 적용한다.


Transformer : Warm-up Learning Rate Scheduler
학습률을 학습동안에 변화되게 한다.
→ 초반에 iteration에 비례하여 학습률을 증가하고
어느 정도 최적점에 다다를 때 학습률을 감소시킨다.


Transformer : Decoder
이전 디코더의 결과 값이 쿼리벡터가 되고
인코더의 결과값이 K, V 벡터가 된다.


Transformer : Masked Self-Attention
디코더의 Attention 입력의 결과로 얻은 히든 벡터에서 
예측 부분의 행렬 요소(아직 생성되지 않아 알 수 없는 부분)를 
0으로 처리하고 0이 아닌 softmax 결과는 재정규화를 한다.

